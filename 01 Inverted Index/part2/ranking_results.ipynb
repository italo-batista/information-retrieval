{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "from inverted_index import get_inverted_index\n",
    "from search_engine import search\n",
    "from tokenizer import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report I reuse some functions and structures from [part one](https://github.com/italo-batista/information-retrieval/blob/master/01%20Inverted%20Index/part1/inverted_index_report.ipynb) (you can click to check documentation). So, I modularized this part one in new modules ([inverted_index](https://github.com/italo-batista/information-retrieval/blob/master/01%20Inverted%20Index/part2/inverted_index.py), [search_engine](https://github.com/italo-batista/information-retrieval/blob/master/01%20Inverted%20Index/part2/search_engine.py) and [tokenizer](https://github.com/italo-batista/information-retrieval/blob/master/01%20Inverted%20Index/part2/tokenizer.py)). I import and use them in this lab. I did so because I wanted to focus on the new strucutres this lab demands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating constants that will be used over this report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMN_AXIS = 1\n",
    "FULL_REPORT_COLNAME = 'noticia'\n",
    "CONTENT_COLNAME = 'conteudo'\n",
    "SUBTITLE_COLNAME = 'subTitulo'\n",
    "TITLE_COLNAME = 'titulo'\n",
    "TOKENS_COLNAME = 'tokens'\n",
    "TERM_COLNAME = 'term'\n",
    "REPORT_ID_COLNAME = 'idNoticia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/estadao_noticias_eleicao.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating alls reports' title and content in just one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenate_report(row):\n",
    "    \"\"\"Concatenate report title and content in just one column.\n",
    "        \n",
    "    Args:\n",
    "        row (:obj: pandas.Series): one row observation from a pandas.DataFrame.            \n",
    "\n",
    "    Return:\n",
    "        str: full report (content with title) in lowercase.\n",
    "    \"\"\"\n",
    "    title = str(row[TITLE_COLNAME])\n",
    "    subtitle = str(row[SUBTITLE_COLNAME])\n",
    "    content = str(row[CONTENT_COLNAME])\n",
    "    full_report = title + \" \" + subtitle + \" \" + content\n",
    "    return full_report.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing content values that are NaN (not a number) for an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_str = \"\"\n",
    "df[TITLE_COLNAME].fillna(empty_str, inplace=True)\n",
    "df[SUBTITLE_COLNAME].fillna(empty_str, inplace=True)\n",
    "df[CONTENT_COLNAME].fillna(empty_str, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[FULL_REPORT_COLNAME] = df.apply(\n",
    "    lambda row: concatenate_report(row), axis=COLUMN_AXIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting just report's id and full content columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[[REPORT_ID_COLNAME, FULL_REPORT_COLNAME]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing report's text and saving tokens in another column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = tokenize(df, FULL_REPORT_COLNAME, TOKENS_COLNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe now looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idNoticia</th>\n",
       "      <th>noticia</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pt espera 30 mil pessoas em festa na esplanada...</td>\n",
       "      <td>[pt, espera, 30, mil, pessoas, em, festa, na, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>alckmin toma posse de olho no planalto governa...</td>\n",
       "      <td>[alckmin, toma, posse, de, olho, no, planalto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>seis obstáculos e desafios do segundo mandato ...</td>\n",
       "      <td>[seis, obstáculos, e, desafios, do, segundo, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>veja as principais fotos do dia e dos eventos...</td>\n",
       "      <td>[veja, as, principais, fotos, do, dia, e, dos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>veja as principais fotos do dia e dos eventos...</td>\n",
       "      <td>[veja, as, principais, fotos, do, dia, e, dos,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idNoticia                                            noticia  \\\n",
       "0          1  pt espera 30 mil pessoas em festa na esplanada...   \n",
       "1          2  alckmin toma posse de olho no planalto governa...   \n",
       "2          3  seis obstáculos e desafios do segundo mandato ...   \n",
       "3          4   veja as principais fotos do dia e dos eventos...   \n",
       "4          5   veja as principais fotos do dia e dos eventos...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [pt, espera, 30, mil, pessoas, em, festa, na, ...  \n",
       "1  [alckmin, toma, posse, de, olho, no, planalto,...  \n",
       "2  [seis, obstáculos, e, desafios, do, segundo, m...  \n",
       "3  [veja, as, principais, fotos, do, dia, e, dos,...  \n",
       "4  [veja, as, principais, fotos, do, dia, e, dos,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_index = get_inverted_index(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Retrieval Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a TermEstimator object to be the expert in calculating some estimates about the terms present in corpus. We will use this TermEstimator instance in our vector space models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TermEstimator:\n",
    "    \n",
    "    def __init__(self, df, inverted_index):\n",
    "        self.df = df\n",
    "        self.big_term_freq_dict = dict()\n",
    "        self.inverted_index = inverted_index\n",
    "        self._calc_terms_frequency()\n",
    "    \n",
    "    def get_tf(self, term, doc_id):\n",
    "        return self.big_term_freq_dict[doc_id][term]\n",
    "    \n",
    "    def calc_idf(self, term):\n",
    "        n_documents = self.get_number_of_docs()\n",
    "        n_containing_term = self.get_number_of_docs_containing(term)\n",
    "        idf = np.log((n_documents + 1) / (n_containing_term))\n",
    "        return idf\n",
    "    \n",
    "    def calc_tfidf(self, term, doc_id):\n",
    "        tf = self.get_tf(term, doc_id)\n",
    "        idf = self.calc_idf(term)\n",
    "        return tf * idf     \n",
    "    \n",
    "    def get_number_of_docs(self):\n",
    "        NUMBER_OF_ROWS_INDEX = 0\n",
    "        n_documents = self.df.shape[NUMBER_OF_ROWS_INDEX] \n",
    "        return n_documents\n",
    "    \n",
    "    def get_number_of_docs_containing(self, term):\n",
    "        return len(self.inverted_index[term].get_docs_ids())\n",
    "    \n",
    "    def _calc_doc_terms_frequency(self, doc_id, doc_terms):\n",
    "        terms_frequencies = collections.Counter(doc_terms)\n",
    "        self.big_term_freq_dict[doc_id] = terms_frequencies\n",
    "    \n",
    "    def _calc_terms_frequency(self):\n",
    "        self.df.apply(\n",
    "            lambda row: self._calc_doc_terms_frequency(\n",
    "                row[REPORT_ID_COLNAME], row[TOKENS_COLNAME]), \n",
    "            axis=COLUMN_AXIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rank search results, we will need to score each doc returned for a search result. So, we will define scorer types. They will be experts in ranking results for a search query. There is the general abstract class Scorer and one class (that will extends Scorer) for each vector model (binary, frequency, idf, bm25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Scorer(ABC):\n",
    "    \n",
    "    def __init__(self, term_estimator):\n",
    "        self.term_estimator = term_estimator\n",
    "\n",
    "    @abstractmethod\n",
    "    def sim(self, query_terms, doc_id):\n",
    "        \"\"\"Get the similarity between a document and a query. This method will be \n",
    "        override by the classes that extends this class, implementing this similarity \n",
    "        function according to the model.\n",
    "\n",
    "        Args:\n",
    "            query_terms (list): query structured as list of terms.            \n",
    "            dos_id (int): id document to get similarity with query.\n",
    "\n",
    "        Return:\n",
    "            float: similarity between document and query.        \n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def ranking_search(self, query, search_result, k):\n",
    "        \"\"\"Rank a search result using the similarity function implemented by\n",
    "        the model that overrides this class.\n",
    "\n",
    "        Args:\n",
    "            query_terms (list): query structured as list of terms.            \n",
    "            search_result (list): list of documents's ids returned by the binary search.\n",
    "            k (int): number of the best ranked documents to return.\n",
    "\n",
    "        Return:\n",
    "            list: list with for the k best ranked documents.        \n",
    "        \"\"\"\n",
    "        ranking = []\n",
    "        query_terms = query.split(\" AND \")\n",
    "        for doc_id in search_result:\n",
    "            score = self.sim(query_terms, doc_id)\n",
    "            ranking.append((doc_id, score))\n",
    "        ranking = sorted(ranking, key=lambda t: t[1], reverse=True)\n",
    "        top_k = ranking[:k]\n",
    "        top_k = list(map(lambda x: x[0], top_k))\n",
    "        return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryScorer(Scorer):\n",
    "    \n",
    "    def __init__(self, term_estimator):\n",
    "        Scorer.__init__(self, term_estimator)\n",
    "        \n",
    "    def sim(self, query_terms, doc_id):\n",
    "        n_matches_with_query = 0\n",
    "        for query_term in query_terms:\n",
    "            if doc_id in self.term_estimator.inverted_index[query_term].get_docs_ids():\n",
    "                n_matches_with_query += 1\n",
    "        return n_matches_with_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrequencyScorer(Scorer):\n",
    "\n",
    "    def __init__(self, term_estimator):\n",
    "        Scorer.__init__(self, term_estimator)\n",
    "        \n",
    "    def sim(self, query_terms, doc_id):\n",
    "        tf_accumulated = 0\n",
    "        for query_term in query_terms:\n",
    "            tf = self.term_estimator.get_tf(query_term, doc_id)\n",
    "            tf_accumulated += tf\n",
    "        return tf_accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrequencyIDFScorer(Scorer):\n",
    "    \n",
    "    def __init__(self, term_estimator):\n",
    "        Scorer.__init__(self, term_estimator)\n",
    "        \n",
    "    def sim(self, query_terms, doc_id):\n",
    "        tfidf_accumulated = 0\n",
    "        for query_term in query_terms:\n",
    "            tfidf = self.term_estimator.calc_tfidf(query_term, doc_id)\n",
    "            tfidf_accumulated += tfidf\n",
    "        return tfidf_accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BM25Scorer(Scorer):\n",
    "    \n",
    "    def __init__(self, term_estimator):\n",
    "        Scorer.__init__(self, term_estimator)\n",
    "        \n",
    "    def sim(self, query_terms, doc_id):\n",
    "        score_accumulated = 0\n",
    "        for query_term in query_terms:\n",
    "            k = np.random.uniform(low=1.2, high=2)\n",
    "            idf = self.term_estimator.calc_idf(query_term)\n",
    "            tf = self.term_estimator.get_tf(query_term, doc_id)\n",
    "            score = idf * (tf * (k+1) / (tf + k))\n",
    "            score_accumulated += score\n",
    "        return score_accumulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making queries.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do five searches using the following queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"segundo turno\",\n",
    "    \"lava jato\",\n",
    "    \"projeto de lei\",\n",
    "    \"compra de voto\",\n",
    "    \"ministério público\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instantiate our expert in calculate estimates about terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_estimator = TermEstimator(df, inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's search!    \n",
    "We will do the same five searches for each scorer model and save its results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_ranked(query, scorer, k=5):\n",
    "    \"\"\"Get boolean query operator.\n",
    "    \n",
    "    Args:\n",
    "        query (str): query to consult using our search engine.\n",
    "        scorer (:Scorer:): model to rank search results.\n",
    "        k (int): number of most ranked documents to return.\n",
    "    \n",
    "    Return:\n",
    "        list: ids for the best ranked documents for a given consult.\n",
    "    \"\"\"     \n",
    "    boolean_query = \" AND \".join(query.split(\" \"))\n",
    "    search_result = search(boolean_query, inverted_index)\n",
    "    ranked_result = scorer.ranking_search(boolean_query, search_result, k)\n",
    "    return ranked_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "For the binary model, we had the following results:\n",
      "\n",
      "\t Query: segundo turno\n",
      "\t Top-5 documents: [2048, 1, 2049, 2050, 4096]\n",
      "\n",
      "\n",
      "\t Query: lava jato\n",
      "\t Top-5 documents: [3, 13, 15, 27, 6177]\n",
      "\n",
      "\n",
      "\t Query: projeto de lei\n",
      "\t Top-5 documents: [3584, 6145, 8194, 3587, 3588]\n",
      "\n",
      "\n",
      "\t Query: compra de voto\n",
      "\t Top-5 documents: [7424, 2178, 5122, 6531, 2311]\n",
      "\n",
      "\n",
      "\t Query: ministério público\n",
      "\t Top-5 documents: [8194, 7, 4104, 8201, 4109]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scorer = BinaryScorer(term_estimator)\n",
    "binary_scorer_results = []\n",
    "for query in queries:\n",
    "    result = search_ranked(query, scorer)\n",
    "    binary_scorer_results.append(result)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('For the binary model, we had the following results:\\n')\n",
    "for query, results in zip(queries, binary_scorer_results):\n",
    "    print('\\t Query:', query)\n",
    "    print('\\t Top-5 documents:', results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "For the TF model, we had the following results:\n",
      "\n",
      "\t Query: segundo turno\n",
      "\t Top-5 documents: [2048, 1, 2049, 2050, 4096]\n",
      "\n",
      "\n",
      "\t Query: lava jato\n",
      "\t Top-5 documents: [3, 13, 15, 27, 6177]\n",
      "\n",
      "\n",
      "\t Query: projeto de lei\n",
      "\t Top-5 documents: [3584, 6145, 8194, 3587, 3588]\n",
      "\n",
      "\n",
      "\t Query: compra de voto\n",
      "\t Top-5 documents: [7424, 2178, 5122, 6531, 2311]\n",
      "\n",
      "\n",
      "\t Query: ministério público\n",
      "\t Top-5 documents: [8194, 7, 4104, 8201, 4109]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scorer = FrequencyScorer(term_estimator)\n",
    "frequency_scorer_results = []\n",
    "for query in querys:\n",
    "    result = search_ranked(query, scorer)\n",
    "    frequency_scorer_results.append(result)\n",
    "    \n",
    "print('---------------------------------------------------------')\n",
    "print('For the TF model, we had the following results:\\n')\n",
    "for query, results in zip(queries, binary_scorer_results):\n",
    "    print('\\t Query:', query)\n",
    "    print('\\t Top-5 documents:', results)\n",
    "    print('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "For the TF-IDF model, we had the following results:\n",
      "\n",
      "\t Query: segundo turno\n",
      "\t Top-5 documents: [2048, 1, 2049, 2050, 4096]\n",
      "\n",
      "\n",
      "\t Query: lava jato\n",
      "\t Top-5 documents: [3, 13, 15, 27, 6177]\n",
      "\n",
      "\n",
      "\t Query: projeto de lei\n",
      "\t Top-5 documents: [3584, 6145, 8194, 3587, 3588]\n",
      "\n",
      "\n",
      "\t Query: compra de voto\n",
      "\t Top-5 documents: [7424, 2178, 5122, 6531, 2311]\n",
      "\n",
      "\n",
      "\t Query: ministério público\n",
      "\t Top-5 documents: [8194, 7, 4104, 8201, 4109]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scorer = FrequencyIDFScorer(term_estimator)\n",
    "frequency_idf_scorer_results = []\n",
    "for query in querys:\n",
    "    result = search_ranked(query, scorer)\n",
    "    frequency_idf_scorer_results.append(result)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('For the TF-IDF model, we had the following results:\\n')\n",
    "for query, results in zip(queries, binary_scorer_results):\n",
    "    print('\\t Query:', query)\n",
    "    print('\\t Top-5 documents:', results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "For the BM25 model, we had the following results:\n",
      "\n",
      "\t Query: segundo turno\n",
      "\t Top-5 documents: [2048, 1, 2049, 2050, 4096]\n",
      "\n",
      "\n",
      "\t Query: lava jato\n",
      "\t Top-5 documents: [3, 13, 15, 27, 6177]\n",
      "\n",
      "\n",
      "\t Query: projeto de lei\n",
      "\t Top-5 documents: [3584, 6145, 8194, 3587, 3588]\n",
      "\n",
      "\n",
      "\t Query: compra de voto\n",
      "\t Top-5 documents: [7424, 2178, 5122, 6531, 2311]\n",
      "\n",
      "\n",
      "\t Query: ministério público\n",
      "\t Top-5 documents: [8194, 7, 4104, 8201, 4109]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scorer = BM25Scorer(term_estimator)\n",
    "bm25_scorer_results = []\n",
    "for query in querys:\n",
    "    result = search_ranked(query, scorer)\n",
    "    bm25_scorer_results.append(result)\n",
    "    \n",
    "print('---------------------------------------------------------')\n",
    "print('For the BM25 model, we had the following results:\\n')\n",
    "for query, results in zip(queries, binary_scorer_results):\n",
    "    print('\\t Query:', query)\n",
    "    print('\\t Top-5 documents:', results)\n",
    "    print('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results with expected answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabarito = pd.read_csv('gabarito.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from average_precision import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BINARY_SEARCH_COL_NAME = 'busca_binaria'\n",
    "TF_COL_NAME = 'tf'\n",
    "TFIDF_COL_NAME = 'tfidf'\n",
    "BM25_COL_NAME = 'bm25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_expected_results(expected_result_type):\n",
    "    \"\"\"Get expected answers for a model in gabarito and change format to list of ints.\"\"\"    \n",
    "    expected_answers = []\n",
    "    from_df = gabarito[expected_result_type]\n",
    "    for query_result in from_df:        \n",
    "        as_str = re.sub('[,\\[\\]]', '', query_result)\n",
    "        as_list = as_str.split(\" \")\n",
    "        list_of_int = list(map(int, as_list))\n",
    "        expected_answers.append(list_of_int)\n",
    "    return expected_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading expected answers for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_binary_scorer_results = get_expected_results(BINARY_SEARCH_COL_NAME)\n",
    "expected_frequency_scorer_results = get_expected_results(TF_COL_NAME)\n",
    "expected_frequency_idf_scorer_results = get_expected_results(TFIDF_COL_NAME)\n",
    "expected_bm25_scorer_results = get_expected_results(BM25_COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing actual and expected results using Mean Average Precision (MAP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP for binary model: 0.92\n",
      "MAP for TF model: 1.00\n",
      "MAP for TF-IDF model: 0.76\n",
      "MAP for BM25 model: 0.53\n"
     ]
    }
   ],
   "source": [
    "map_binary = mapk(expected_binary_scorer_results, binary_scorer_results, k=5)\n",
    "print('MAP for binary model: {:.2f}'.format(map_binary))\n",
    "\n",
    "map_tf = mapk(expected_frequency_scorer_results, frequency_scorer_results, k=5)\n",
    "print('MAP for TF model: {:.2f}'.format(map_tf))\n",
    "\n",
    "map_tfidf = mapk(expected_frequency_idf_scorer_results, frequency_idf_scorer_results, k=5)\n",
    "print('MAP for TF-IDF model: {:.2f}'.format(map_tfidf))\n",
    "\n",
    "map_bm25 = mapk(expected_bm25_scorer_results, bm25_scorer_results, k=5)\n",
    "print('MAP for BM25 model: {:.2f}'.format(map_bm25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that my implemented models give similar results to the expected. Some little differences, as idf or tf or bm25 formula, even text cleanup and standardization (preprocessing stuff), may have impacted in MAP scores, but the scorer still make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing with google results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GOOGLE_COL_NAME = 'google'\n",
    "google_results = get_expected_results(GOOGLE_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP for binary model: 0.00\n",
      "MAP for TF model: 0.05\n",
      "MAP for TF-IDF model: 0.08\n",
      "MAP for BM25 model: 0.20\n"
     ]
    }
   ],
   "source": [
    "map_binary = mapk(google_results, binary_scorer_results, k=5)\n",
    "print('MAP for binary model: {:.2f}'.format(map_binary))\n",
    "\n",
    "map_tf = mapk(google_results, frequency_scorer_results, k=5)\n",
    "print('MAP for TF model: {:.2f}'.format(map_tf))\n",
    "\n",
    "map_tfidf = mapk(google_results, frequency_idf_scorer_results, k=5)\n",
    "print('MAP for TF-IDF model: {:.2f}'.format(map_tfidf))\n",
    "\n",
    "map_bm25 = mapk(google_results, bm25_scorer_results, k=5)\n",
    "print('MAP for BM25 model: {:.2f}'.format(map_bm25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models' results were inverse to its complexity. For example, the least complexe model, binary, gave worst results. And the most complexe model, bm25, gave best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
